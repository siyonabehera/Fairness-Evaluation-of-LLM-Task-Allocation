{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Please avoid running this pipeline multiple times as there is a budget set of $5 for the API keys of the two LLM's used here.\n",
        "- Ensure you provide an instances.json file. There is an example file that you can find below."
      ],
      "metadata": {
        "id": "FYZFNZ2HR_c1"
      },
      "id": "FYZFNZ2HR_c1"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LNgQFCvSR_N2"
      },
      "id": "LNgQFCvSR_N2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic"
      ],
      "metadata": {
        "id": "rhPq0areNCCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1361c4a5-12e9-4a3b-8281-7a2023ded555"
      },
      "id": "rhPq0areNCCR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.75.0-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n",
            "Downloading anthropic-0.75.0-py3-none-any.whl (388 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.2/388.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.75.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Put your data payload in content/instances.json\n",
        "# I put an example payload\n",
        "[\n",
        "  {\n",
        "    \"instance_id\": \"inst1\",\n",
        "    \"agents\": [\"A1\", \"A2\", \"A3\"],\n",
        "    \"chores\": [\"C1\", \"C2\", \"C3\"],\n",
        "    \"valuations\": {\n",
        "      \"A1\": { \"C1\": -4, \"C2\": -2, \"C3\": -6 },\n",
        "      \"A2\": { \"C1\": -5, \"C2\": -1, \"C3\": -3 },\n",
        "      \"A3\": { \"C1\": -3, \"C2\": -4, \"C3\": -2 }\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"instance_id\": \"inst2\",\n",
        "    \"agents\": [\"A1\", \"A2\", \"A3\"],\n",
        "    \"chores\": [\"C1\", \"C2\", \"C3\", \"C4\"],\n",
        "    \"valuations\": {\n",
        "      \"A1\": { \"C1\": -2, \"C2\": -6, \"C3\": -3, \"C4\": -5 },\n",
        "      \"A2\": { \"C1\": -4, \"C2\": -3, \"C3\": -5, \"C4\": -1 },\n",
        "      \"A3\": { \"C1\": -6, \"C2\": -2, \"C3\": -4, \"C4\": -3 }\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"instance_id\": \"inst3\",\n",
        "    \"agents\": [\"A1\", \"A2\", \"A3\", \"A4\"],\n",
        "    \"chores\": [\"C1\", \"C2\", \"C3\"],\n",
        "    \"valuations\": {\n",
        "      \"A1\": { \"C1\": -3, \"C2\": -5, \"C3\": -1 },\n",
        "      \"A2\": { \"C1\": -1, \"C2\": -4, \"C3\": -6 },\n",
        "      \"A3\": { \"C1\": -5, \"C2\": -2, \"C3\": -4 },\n",
        "      \"A4\": { \"C1\": -2, \"C2\": -3, \"C3\": -5 }\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"instance_id\": \"inst4\",\n",
        "    \"agents\": [\"A1\", \"A2\", \"A3\"],\n",
        "    \"chores\": [\"C1\", \"C2\", \"C3\", \"C4\", \"C5\"],\n",
        "    \"valuations\": {\n",
        "      \"A1\": { \"C1\": -3, \"C2\": -1, \"C3\": -4, \"C4\": -6, \"C5\": -2 },\n",
        "      \"A2\": { \"C1\": -2, \"C2\": -5, \"C3\": -3, \"C4\": -1, \"C5\": -4 },\n",
        "      \"A3\": { \"C1\": -4, \"C2\": -2, \"C3\": -5, \"C4\": -3, \"C5\": -1 }\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"instance_id\": \"inst5\",\n",
        "    \"agents\": [\"A1\", \"A2\", \"A3\", \"A4\"],\n",
        "    \"chores\": [\"C1\", \"C2\", \"C3\", \"C4\"],\n",
        "    \"valuations\": {\n",
        "      \"A1\": { \"C1\": -6, \"C2\": -3, \"C3\": -2, \"C4\": -4 },\n",
        "      \"A2\": { \"C1\": -3, \"C2\": -5, \"C3\": -4, \"C4\": -1 },\n",
        "      \"A3\": { \"C1\": -2, \"C2\": -4, \"C3\": -6, \"C4\": -3 },\n",
        "      \"A4\": { \"C1\": -4, \"C2\": -1, \"C3\": -3, \"C4\": -5 }\n",
        "    }\n",
        "  }\n",
        "]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4E7KAQnM9F_",
        "outputId": "6b1e786d-db1b-41c0-9653-2d6d5542005d"
      },
      "id": "n4E7KAQnM9F_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'instance_id': 'inst1',\n",
              "  'agents': ['A1', 'A2', 'A3'],\n",
              "  'chores': ['C1', 'C2', 'C3'],\n",
              "  'valuations': {'A1': {'C1': -4, 'C2': -2, 'C3': -6},\n",
              "   'A2': {'C1': -5, 'C2': -1, 'C3': -3},\n",
              "   'A3': {'C1': -3, 'C2': -4, 'C3': -2}}},\n",
              " {'instance_id': 'inst2',\n",
              "  'agents': ['A1', 'A2', 'A3'],\n",
              "  'chores': ['C1', 'C2', 'C3', 'C4'],\n",
              "  'valuations': {'A1': {'C1': -2, 'C2': -6, 'C3': -3, 'C4': -5},\n",
              "   'A2': {'C1': -4, 'C2': -3, 'C3': -5, 'C4': -1},\n",
              "   'A3': {'C1': -6, 'C2': -2, 'C3': -4, 'C4': -3}}},\n",
              " {'instance_id': 'inst3',\n",
              "  'agents': ['A1', 'A2', 'A3', 'A4'],\n",
              "  'chores': ['C1', 'C2', 'C3'],\n",
              "  'valuations': {'A1': {'C1': -3, 'C2': -5, 'C3': -1},\n",
              "   'A2': {'C1': -1, 'C2': -4, 'C3': -6},\n",
              "   'A3': {'C1': -5, 'C2': -2, 'C3': -4},\n",
              "   'A4': {'C1': -2, 'C2': -3, 'C3': -5}}},\n",
              " {'instance_id': 'inst4',\n",
              "  'agents': ['A1', 'A2', 'A3'],\n",
              "  'chores': ['C1', 'C2', 'C3', 'C4', 'C5'],\n",
              "  'valuations': {'A1': {'C1': -3, 'C2': -1, 'C3': -4, 'C4': -6, 'C5': -2},\n",
              "   'A2': {'C1': -2, 'C2': -5, 'C3': -3, 'C4': -1, 'C5': -4},\n",
              "   'A3': {'C1': -4, 'C2': -2, 'C3': -5, 'C4': -3, 'C5': -1}}},\n",
              " {'instance_id': 'inst5',\n",
              "  'agents': ['A1', 'A2', 'A3', 'A4'],\n",
              "  'chores': ['C1', 'C2', 'C3', 'C4'],\n",
              "  'valuations': {'A1': {'C1': -6, 'C2': -3, 'C3': -2, 'C4': -4},\n",
              "   'A2': {'C1': -3, 'C2': -5, 'C3': -4, 'C4': -1},\n",
              "   'A3': {'C1': -2, 'C2': -4, 'C3': -6, 'C4': -3},\n",
              "   'A4': {'C1': -4, 'C2': -1, 'C3': -3, 'C4': -5}}}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TkiqFnkgI2px"
      },
      "id": "TkiqFnkgI2px"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2y8WwsbpR9NH"
      },
      "id": "2y8WwsbpR9NH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13422a31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13422a31",
        "outputId": "44b2cd4e-c783-42d5-d3ea-dc650b851e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Milestone 2 prototype on 5 instances, 10 samples each, 2 LLMs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Instance inst1, LLM gpt4o: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\n",
            "Instance inst1, LLM claude-sonnet-4-5: 100%|██████████| 10/10 [00:28<00:00,  2.89s/it]\n",
            "Instance inst2, LLM gpt4o: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n",
            "Instance inst2, LLM claude-sonnet-4-5:  40%|████      | 4/10 [00:10<00:16,  2.69s/it]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import itertools\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Any\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import userdata;\n",
        "\n",
        "\n",
        "# note: for the OpenAPI models we just use the API endpoint rather than the SDK.\n",
        "import anthropic\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# importing secrets\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY');\n",
        "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY');\n",
        "\n",
        "\n",
        "\n",
        "# choose which LLMs you actually want to run.\n",
        "# Hopefully, sharing this file means you have access to the keys we've added.\n",
        "LLM_CONFIG = {\n",
        "    \"gpt4o\": {\n",
        "        \"provider\": \"openai\",\n",
        "        \"model\": \"gpt-4o-mini\",\n",
        "        \"enabled\": True\n",
        "    },\n",
        "    \"claude-sonnet-4-5\": {\n",
        "        \"provider\": \"anthropic\",\n",
        "        \"model\": \"claude-sonnet-4-5\",\n",
        "        \"enabled\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# load da instances file here\n",
        "INSTANCES_PATH = \"./content/instances.json\"\n",
        "\n",
        "# how many  instances and samples to run?\n",
        "N_INSTANCES = 24      # first 5 instances in your file\n",
        "N_SAMPLES_PER_INST = 10   # approx requirement from spec\n",
        "TEMPERATURE = 0.7\n",
        "\n",
        "## data structs + loaders\n",
        "\n",
        "@dataclass\n",
        "class ChoreInstance:\n",
        "    instance_id: str\n",
        "    agents: List[str]\n",
        "    chores: List[str]\n",
        "    valuations: Dict[str, Dict[str, float]]  # valuations[agent][chore]\n",
        "\n",
        "def load_instances(path: str) -> List[ChoreInstance]:\n",
        "    \"\"\"Load allocation instances from a JSON file.\"\"\"\n",
        "    with open(path, \"r\") as f:\n",
        "        raw = json.load(f)\n",
        "\n",
        "    instances: List[ChoreInstance] = []\n",
        "    for inst in raw:\n",
        "        instances.append(\n",
        "            ChoreInstance(\n",
        "                instance_id=inst[\"instance_id\"],\n",
        "                agents=inst[\"agents\"],\n",
        "                chores=inst[\"chores\"],\n",
        "                valuations=inst[\"valuations\"]\n",
        "            )\n",
        "        )\n",
        "    return instances\n",
        "\n",
        "\n",
        "# prompt gens\n",
        "def build_prompt(instance: ChoreInstance,\n",
        "                 mode: str = \"generate\",\n",
        "                 phrasing: str = \"negative_disutility\") -> str:\n",
        "    \"\"\"\n",
        "    Build a prompt for a single instance.\n",
        "\n",
        "    mode: \"generate\" or \"select\" (project spec).\n",
        "    phrasing:\n",
        "        - \"negative_disutility\": valuations stored as negative numbers (more negative = worse)\n",
        "        - \"positive_cost\": valuations stored as positive costs (larger = worse)\n",
        "    \"\"\"\n",
        "\n",
        "    agents_str = \", \".join(instance.agents)\n",
        "    chores_str = \", \".join(instance.chores)\n",
        "\n",
        "    # talbe of valuations in text\n",
        "    table_lines = []\n",
        "    header = \"Chore valuations (numbers indicate how bad each chore is for each agent):\"\n",
        "    table_lines.append(header)\n",
        "    table_lines.append(\"Agent | \" + \" | \".join(instance.chores))\n",
        "    table_lines.append(\"-\" * (7 + 4 * len(instance.chores)))\n",
        "\n",
        "    for a in instance.agents:\n",
        "        row_vals = [str(instance.valuations[a][c]) for c in instance.chores]\n",
        "        table_lines.append(f\"{a} | \" + \" | \".join(row_vals))\n",
        "\n",
        "    table_text = \"\\n\".join(table_lines)\n",
        "\n",
        "    if phrasing == \"negative_disutility\":\n",
        "        meaning_text = (\n",
        "            \"The numbers are negative disutilities. A more negative number means the chore is worse for that agent.\\n\"\n",
        "            \"The goal is to allocate chores so that agents are treated fairly and total burden is not too high.\"\n",
        "        )\n",
        "    else:\n",
        "        meaning_text = (\n",
        "            \"The numbers are positive costs. A larger number means the chore is worse for that agent.\\n\"\n",
        "            \"The goal is to allocate chores so that agents are treated fairly and total burden is not too high.\"\n",
        "        )\n",
        "\n",
        "    if mode == \"generate\":\n",
        "        task_text = (\n",
        "            \"Your task is to PROPOSE a fair allocation of chores.\\n\"\n",
        "            \"You may assign each chore to exactly one agent, or to 'None' if it is discarded.\\n\"\n",
        "            \"Return ONLY a valid JSON object in this exact format:\\n\\n\"\n",
        "            \"{\\n\"\n",
        "            '  \"allocation\": {\\n'\n",
        "            '    \"C1\": \"A1\",\\n'\n",
        "            '    \"C2\": \"A2\",\\n'\n",
        "            '    \"C3\": \"None\"\\n'\n",
        "            \"  }\\n\"\n",
        "            \"}\\n\\n\"\n",
        "            \"Use only agent names or the string \\\"None\\\" as values. Do not include any extra text.\"\n",
        "        )\n",
        "    else:  # \"select\" mode\n",
        "        task_text = (\n",
        "            \"Your task is to choose a fair allocation of chores.\\n\"\n",
        "            \"You may assign each chore to exactly one agent, or to 'None' if it is discarded.\\n\"\n",
        "            \"Return ONLY a valid JSON object in this exact format:\\n\\n\"\n",
        "            \"{\\n\"\n",
        "            '  \"allocation\": {\\n'\n",
        "            '    \"C1\": \"A1\",\\n'\n",
        "            '    \"C2\": \"A2\",\\n'\n",
        "            '    \"C3\": \"None\"\\n'\n",
        "            \"  }\\n\"\n",
        "            \"}\\n\\n\"\n",
        "            \"Use only agent names or the string \\\"None\\\" as values. Do not include any extra text.\"\n",
        "        )\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "We are allocating chores (negatively valued items) among agents.\n",
        "\n",
        "Agents: {agents_str}\n",
        "Chores: {chores_str}\n",
        "\n",
        "{table_text}\n",
        "\n",
        "{meaning_text}\n",
        "\n",
        "{task_text}\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "## llm clients\n",
        "class BaseLLMClient:\n",
        "    def generate(self, prompt: str, temperature: float = 1.0) -> str:\n",
        "        raise NotImplementedError\n",
        "\n",
        "## looking back, using the openai sdk would have simplified a lot of this...\n",
        "class OpenAIChatClient(BaseLLMClient):\n",
        "    def __init__(self, api_key: str, model: str):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "\n",
        "    def generate(self, prompt: str, temperature: float = 1.0) -> str:\n",
        "        url = \"https://api.openai.com/v1/chat/completions\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        payload = {\n",
        "            \"model\": self.model,\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a careful algorithm designer who strictly follows the output format.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            \"temperature\": temperature\n",
        "        }\n",
        "        resp = requests.post(url, headers=headers, json=payload, timeout=60)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "class AnthropicChatClient(BaseLLMClient):\n",
        "    def __init__(self, api_key: str, model: str):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
        "\n",
        "    def generate(self, prompt: str, temperature: float = 1.0) -> str:\n",
        "        \"\"\"\n",
        "        Uses the official Anthropic Python SDK.\n",
        "        Matches the Messages API examples in the docs.\n",
        "        \"\"\"\n",
        "        message = self.client.messages.create(\n",
        "            model=self.model,                        # \"claude-sonnet-4-5\"\n",
        "            max_tokens=512,\n",
        "            temperature=temperature,\n",
        "            system=\"You are a careful algorithm designer who strictly follows the output format.\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Extract only text blocks from the response\n",
        "        chunks = []\n",
        "        for block in message.content:\n",
        "            if block.type == \"text\":\n",
        "                chunks.append(block.text)\n",
        "\n",
        "        return \"\".join(chunks)\n",
        "\n",
        "\n",
        "# init clients + catch key related errors\n",
        "def make_llm_clients(config: Dict[str, Any]) -> Dict[str, BaseLLMClient]:\n",
        "    clients = {}\n",
        "    for name, cfg in config.items():\n",
        "        if not cfg.get(\"enabled\", False):\n",
        "            continue\n",
        "        provider = cfg[\"provider\"]\n",
        "        if provider == \"openai\":\n",
        "            if OPENAI_API_KEY in (None, \"\", \"YOUR_OPENAI_KEY_HERE\"):\n",
        "                print(f\"[WARN] OpenAI key missing; skipping {name}\")\n",
        "                continue\n",
        "            clients[name] = OpenAIChatClient(OPENAI_API_KEY, cfg[\"model\"])\n",
        "        elif provider == \"anthropic\":\n",
        "            if ANTHROPIC_API_KEY in (None, \"\", \"YOUR_ANTHROPIC_KEY_HERE\"):\n",
        "                print(f\"[WARN] Anthropic key missing; skipping {name}\")\n",
        "                continue\n",
        "            clients[name] = AnthropicChatClient(ANTHROPIC_API_KEY, cfg[\"model\"])\n",
        "        else:\n",
        "            print(f\"[WARN] Unknown provider {provider}; skipping {name}\")\n",
        "    return clients\n",
        "\n",
        "\n",
        "# parsing allocs\n",
        "\n",
        "def extract_json_from_text(text: str) -> Optional[dict]:\n",
        "    \"\"\"Robustly pull the first JSON object from a text response.\"\"\"\n",
        "    first = text.find(\"{\")\n",
        "    last = text.rfind(\"}\")\n",
        "    if first == -1 or last == -1 or last <= first:\n",
        "        return None\n",
        "    snippet = text[first:last+1]\n",
        "    try:\n",
        "        return json.loads(snippet)\n",
        "    except json.JSONDecodeError:\n",
        "        return None\n",
        "\n",
        "def parse_allocation(text: str, chores: List[str]) -> Optional[Dict[str, str]]:\n",
        "    \"\"\"Parse model output into an allocation dict: {chore -> agent_or_None}.\"\"\"\n",
        "    data = extract_json_from_text(text)\n",
        "    if data is None or \"allocation\" not in data:\n",
        "        return None\n",
        "    alloc = data[\"allocation\"]\n",
        "    if not all(c in alloc for c in chores):\n",
        "        return None\n",
        "    return {c: str(alloc[c]) for c in chores}\n",
        "\n",
        "\n",
        "# fairness and effiency checks\n",
        "\n",
        "\n",
        "def normalized_cost(v: float) -> float:\n",
        "    \"\"\"Convert valuations to a comparable 'cost'. If v is negative, cost=-v; else cost=v.\"\"\"\n",
        "    return -v if v < 0 else v\n",
        "\n",
        "def compute_agent_costs(instance: ChoreInstance,\n",
        "                        allocation: Dict[str, str]) -> Dict[str, float]:\n",
        "    \"\"\"Total cost each agent gets under an allocation.\"\"\"\n",
        "    costs = {a: 0.0 for a in instance.agents}\n",
        "    for chore, agent in allocation.items():\n",
        "        if agent in instance.agents:\n",
        "            v = instance.valuations[agent][chore]\n",
        "            costs[agent] += normalized_cost(v)\n",
        "    return costs\n",
        "\n",
        "def is_envy_free(instance: ChoreInstance,\n",
        "                 allocation: Dict[str, str]) -> bool:\n",
        "    \"\"\"Envy-freeness for chores.\"\"\"\n",
        "    costs = compute_agent_costs(instance, allocation)\n",
        "    bundles = {a: [c for c, ag in allocation.items() if ag == a] for a in instance.agents}\n",
        "\n",
        "    for i in instance.agents:\n",
        "        own_cost = sum(normalized_cost(instance.valuations[i][c]) for c in bundles[i])\n",
        "        for j in instance.agents:\n",
        "            if i == j:\n",
        "                continue\n",
        "            other_cost_if_i_had_j = sum(normalized_cost(instance.valuations[i][c]) for c in bundles[j])\n",
        "            if own_cost > other_cost_if_i_had_j + 1e-9:\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "def is_equitable(instance: ChoreInstance,\n",
        "                 allocation: Dict[str, str],\n",
        "                 tol: float = 1e-9) -> bool:\n",
        "    \"\"\"Equitability: all agents have (approximately) equal cost.\"\"\"\n",
        "    costs = compute_agent_costs(instance, allocation)\n",
        "    vals = list(costs.values())\n",
        "    return max(vals) - min(vals) <= tol\n",
        "\n",
        "def rawlsian_cost(instance: ChoreInstance,\n",
        "                  allocation: Dict[str, str]) -> float:\n",
        "    \"\"\"Rawlsian maximin: cost of the worst-off agent (highest cost).\"\"\"\n",
        "    costs = compute_agent_costs(instance, allocation)\n",
        "    return max(costs.values()) if costs else 0.0\n",
        "\n",
        "def total_cost(instance: ChoreInstance,\n",
        "               allocation: Dict[str, str]) -> float:\n",
        "    \"\"\"Social cost = sum of agents' costs.\"\"\"\n",
        "    costs = compute_agent_costs(instance, allocation)\n",
        "    return sum(costs.values())\n",
        "\n",
        "def all_allocations(instance: ChoreInstance,\n",
        "                    allow_discard: bool = True):\n",
        "    \"\"\"Generate all possible allocations for brute-force checks.\"\"\"\n",
        "    choices = list(instance.agents)\n",
        "    if allow_discard:\n",
        "        choices.append(\"None\")\n",
        "    for combo in itertools.product(choices, repeat=len(instance.chores)):\n",
        "        yield {chore: assignee for chore, assignee in zip(instance.chores, combo)}\n",
        "\n",
        "def is_pareto_optimal(instance: ChoreInstance,\n",
        "                      allocation: Dict[str, str],\n",
        "                      allow_discard: bool = True) -> bool:\n",
        "    \"\"\"Pareto optimal for chores.\"\"\"\n",
        "    current_costs = compute_agent_costs(instance, allocation)\n",
        "    agents = instance.agents\n",
        "\n",
        "    for alt in all_allocations(instance, allow_discard=allow_discard):\n",
        "        if alt == allocation:\n",
        "            continue\n",
        "        alt_costs = compute_agent_costs(instance, alt)\n",
        "\n",
        "        weakly_better = all(alt_costs[a] <= current_costs[a] + 1e-9 for a in agents)\n",
        "        strictly_better = any(alt_costs[a] < current_costs[a] - 1e-9 for a in agents)\n",
        "\n",
        "        if weakly_better and strictly_better:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_socially_optimal(instance: ChoreInstance,\n",
        "                        allocation: Dict[str, str],\n",
        "                        allow_discard: bool = True) -> bool:\n",
        "    \"\"\"Socially optimal (utilitarian): allocation has minimal total cost.\"\"\"\n",
        "    current_tc = total_cost(instance, allocation)\n",
        "\n",
        "    for alt in all_allocations(instance, allow_discard=allow_discard):\n",
        "        if alt == allocation:\n",
        "            continue\n",
        "        if total_cost(instance, alt) < current_tc - 1e-9:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "# running experiments\n",
        "\n",
        "def run_single_query(instance: ChoreInstance,\n",
        "                     llm_name: str,\n",
        "                     llm_client: BaseLLMClient,\n",
        "                     mode: str = \"generate\",\n",
        "                     phrasing: str = \"negative_disutility\",\n",
        "                     temperature: float = 1.0) -> Dict[str, Any]:\n",
        "    \"\"\"Send one prompt, parse response, and evaluate fairness/efficiency.\"\"\"\n",
        "    prompt = build_prompt(instance, mode=mode, phrasing=phrasing)\n",
        "    raw_response = llm_client.generate(prompt, temperature=temperature)\n",
        "\n",
        "    allocation = parse_allocation(raw_response, instance.chores)\n",
        "\n",
        "    result = {\n",
        "        \"instance_id\": instance.instance_id,\n",
        "        \"llm\": llm_name,\n",
        "        \"mode\": mode,\n",
        "        \"phrasing\": phrasing,\n",
        "        \"raw_response\": raw_response,\n",
        "        \"parsed_ok\": allocation is not None\n",
        "    }\n",
        "\n",
        "    if allocation is None:\n",
        "        result.update({\n",
        "            \"envy_free\": None,\n",
        "            \"equitable\": None,\n",
        "            \"rawlsian_cost\": None,\n",
        "            \"total_cost\": None,\n",
        "            \"pareto_optimal\": None,\n",
        "            \"socially_optimal\": None\n",
        "        })\n",
        "    else:\n",
        "        result.update({\n",
        "            \"envy_free\": is_envy_free(instance, allocation),\n",
        "            \"equitable\": is_equitable(instance, allocation),\n",
        "            \"rawlsian_cost\": rawlsian_cost(instance, allocation),\n",
        "            \"total_cost\": total_cost(instance, allocation),\n",
        "            \"pareto_optimal\": is_pareto_optimal(instance, allocation, allow_discard=False),\n",
        "            \"socially_optimal\": is_socially_optimal(instance, allocation, allow_discard=False),\n",
        "            \"allocation_json\": json.dumps(allocation)\n",
        "        })\n",
        "    return result\n",
        "\n",
        "\n",
        "def run_milestone2_experiments():\n",
        "    # 1) load data\n",
        "    instances = load_instances(INSTANCES_PATH)\n",
        "    if not instances:\n",
        "        raise ValueError(\"No instances loaded. Check INSTANCES_PATH and file format.\")\n",
        "\n",
        "    # 2) take first N_INSTANCES\n",
        "    instances_subset = instances[:N_INSTANCES]\n",
        "\n",
        "    # 3) build LLM clients\n",
        "    llm_clients = make_llm_clients(LLM_CONFIG)\n",
        "    if not llm_clients:\n",
        "        raise ValueError(\"No LLM clients are enabled or properly configured.\")\n",
        "\n",
        "    print(f\"Running Milestone 2 prototype on {len(instances_subset)} instances, \"\n",
        "          f\"{N_SAMPLES_PER_INST} samples each, {len(llm_clients)} LLMs.\")\n",
        "\n",
        "    rows = []\n",
        "    for inst in instances_subset:\n",
        "        for llm_name, client in llm_clients.items():\n",
        "            for s in tqdm(range(N_SAMPLES_PER_INST),\n",
        "                          desc=f\"Instance {inst.instance_id}, LLM {llm_name}\"):\n",
        "                out = run_single_query(\n",
        "                    instance=inst,\n",
        "                    llm_name=llm_name,\n",
        "                    llm_client=client,\n",
        "                    mode=\"generate\",             # you can also test \"select\"\n",
        "                    phrasing=\"negative_disutility\",\n",
        "                    temperature=TEMPERATURE\n",
        "                )\n",
        "                rows.append(out)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(\"/content/milestone2_results.csv\", index=False)\n",
        "    print(\"Saved results to /content/milestone2_results.csv\")\n",
        "    return df\n",
        "\n",
        "# NOTE CONSOLE WILL NOT OUTPUT EVERYTHING, PLEASE CHECK THE FILES GENERATED.\n",
        "\n",
        "df_results = run_milestone2_experiments()\n",
        "df_results.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# below is jake's code that was converted from R to Python\n",
        "import pandas as pd\n",
        "\n",
        "# load\n",
        "data = pd.read_csv(\"/content/milestone2_results.csv\")\n",
        "\n",
        "\n",
        "data2 = data.dropna()\n",
        "\n",
        "print(\"nrow(data):\", len(data))\n",
        "print(\"nrow(data2):\", len(data2))\n",
        "\n",
        "\n",
        "bool_cols = [\"envy_free\", \"equitable\", \"pareto_optimal\", \"socially_optimal\"]\n",
        "true_set = {True, \"True\", \"true\", 1, 1.0}\n",
        "\n",
        "for c in bool_cols:\n",
        "    if c in data2.columns:\n",
        "        data2[c] = data2[c].apply(lambda x: x in true_set)\n",
        "\n",
        "\n",
        "mask_all_false = (~data2[\"envy_free\"]) & (~data2[\"equitable\"]) & (~data2[\"pareto_optimal\"]) & (~data2[\"socially_optimal\"])\n",
        "count_all_false = int(mask_all_false.sum())\n",
        "\n",
        "print(\"count_all_false:\", count_all_false)\n",
        "print(\"nrow(data2) - count_all_false:\", len(data2) - count_all_false)\n",
        "\n",
        "mask_any_true = data2[\"envy_free\"] | data2[\"equitable\"] | data2[\"pareto_optimal\"] | data2[\"socially_optimal\"]\n",
        "data_true = data2.loc[mask_any_true].copy()\n",
        "\n",
        "print(\"nrow(data_true):\", len(data_true))\n",
        "\n",
        "\n",
        "summary = {\n",
        "    \"envy_free\": int(data_true[\"envy_free\"].sum()),\n",
        "    \"equitable\": int(data_true[\"equitable\"].sum()),\n",
        "    \"pareto_optimal\": int(data_true[\"pareto_optimal\"].sum()),\n",
        "    \"socially_optimal\": int(data_true[\"socially_optimal\"].sum()),\n",
        "}\n",
        "\n",
        "print(\"summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ7PQndsI3w9",
        "outputId": "729568be-4dd5-4c48-f3bb-046f2bc5d46b"
      },
      "id": "GQ7PQndsI3w9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nrow(data): 100\n",
            "nrow(data2): 92\n",
            "count_all_false: 46\n",
            "nrow(data2) - count_all_false: 46\n",
            "nrow(data_true): 46\n",
            "summary:\n",
            "{'envy_free': 10, 'equitable': 0, 'pareto_optimal': 46, 'socially_optimal': 20}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1133631521.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data2[c] = data2[c].apply(lambda x: x in true_set)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}